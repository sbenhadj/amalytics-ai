{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Inf√©rence LLaMA + LoRA - Notebook Colab Pr√™t √† l'Emploi\n",
        "\n",
        "Ce notebook teste l'inf√©rence avec votre mod√®le LLaMA fine-tun√© avec LoRA.\n",
        "\n",
        "## üìã Instructions rapides\n",
        "\n",
        "1. **Activer le GPU** : Runtime ‚Üí Change runtime type ‚Üí GPU (T4 ou A100)\n",
        "2. **Remplacer les chemins** dans les cellules ci-dessous (marqu√©s avec ‚ö†Ô∏è)\n",
        "3. **Ex√©cuter les cellules** dans l'ordre (Shift+Enter)\n",
        "\n",
        "## üìù Checklist\n",
        "\n",
        "- [ ] GPU activ√©\n",
        "- [ ] Token Hugging Face pr√™t\n",
        "- [ ] Mod√®le LoRA disponible (Google Drive, Hugging Face, ou upload)\n",
        "- [ ] PDF de test + template JSON pr√™ts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 1 : V√©rifier le GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå ATTENTION: Aucun GPU d√©tect√©!\")\n",
        "    print(\"   Allez dans Runtime ‚Üí Change runtime type ‚Üí GPU (T4 ou A100)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 2 : Installer les d√©pendances depuis requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option A: T√©l√©charger requirements.txt depuis votre repo GitHub\n",
        "# D√©commentez et modifiez l'URL si vous avez le fichier sur GitHub\n",
        "# !wget -q https://raw.githubusercontent.com/VOTRE_USERNAME/VOTRE_REPO/main/amalytics-ml/code/requirements.txt -O /content/requirements.txt\n",
        "\n",
        "# Option B: Cr√©er requirements.txt directement avec toutes les d√©pendances\n",
        "requirements_content = \"\"\"torch\n",
        "transformers\n",
        "accelerate\n",
        "bitsandbytes\n",
        "peft\n",
        "datasets\n",
        "trl\n",
        "pdfplumber\n",
        "pyffx\n",
        "scikit-learn\n",
        "natsort\n",
        "fpdf2\n",
        "lxml\n",
        "nltk\n",
        "huggingface_hub\"\"\"\n",
        "\n",
        "# V√©rifier si requirements.txt existe d√©j√†, sinon le cr√©er\n",
        "import os\n",
        "req_file = \"/content/requirements.txt\"\n",
        "if not os.path.exists(req_file):\n",
        "    with open(req_file, \"w\") as f:\n",
        "        f.write(requirements_content)\n",
        "    print(\"‚úÖ requirements.txt cr√©√© localement\")\n",
        "else:\n",
        "    print(\"‚úÖ requirements.txt trouv√© (probablement t√©l√©charg√© depuis GitHub)\")\n",
        "\n",
        "print(\"\\nüì¶ Installation des d√©pendances depuis requirements.txt...\")\n",
        "print(\"   (Cela peut prendre 5-10 minutes, soyez patient...)\")\n",
        "\n",
        "# Installer depuis requirements.txt\n",
        "!pip install -q -r /content/requirements.txt\n",
        "\n",
        "print(\"\\n‚úÖ Toutes les d√©pendances sont install√©es!\")\n",
        "print(\"\\nüìã Packages install√©s:\")\n",
        "!pip list | grep -E \"(torch|transformers|peft|bitsandbytes|pdfplumber)\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 3 : Authentification Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# ‚ö†Ô∏è REMPLACER PAR VOTRE TOKEN Hugging Face\n",
        "# Obtenez votre token sur: https://huggingface.co/settings/tokens\n",
        "HF_TOKEN = \"VOTRE_TOKEN_HF\"  # Token commen√ßant par hf_...\n",
        "\n",
        "login(HF_TOKEN)\n",
        "print(\"‚úÖ Authentification Hugging Face r√©ussie\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 4 : T√©l√©charger/Uploadez votre code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# ============================================\n",
        "# OPTION A : Cloner depuis GitHub (RECOMMAND√â)\n",
        "# ============================================\n",
        "# ‚ö†Ô∏è MODIFIER : Remplacez par l'URL de votre repository GitHub\n",
        "GITHUB_REPO_URL = \"https://github.com/VOTRE_USERNAME/amalytics-ml.git\"  # ‚ö†Ô∏è MODIFIER\n",
        "\n",
        "# Si le repo est private, utilisez un token:\n",
        "# GITHUB_REPO_URL = \"https://VOTRE_TOKEN@github.com/VOTRE_USERNAME/amalytics-ml.git\"\n",
        "\n",
        "# Cloner depuis GitHub\n",
        "if not os.path.exists(\"/content/amalytics-ml\"):\n",
        "    print(f\"üì• Clonage depuis GitHub: {GITHUB_REPO_URL}\")\n",
        "    !git clone {GITHUB_REPO_URL} /content/amalytics-ml\n",
        "    print(\"‚úÖ Code clon√© depuis GitHub!\")\n",
        "else:\n",
        "    print(\"‚úÖ Le repository existe d√©j√† (peut-√™tre d√©j√† clon√©)\")\n",
        "\n",
        "# ============================================\n",
        "# OPTION B : Upload manuel (si vous n'utilisez pas GitHub)\n",
        "# ============================================\n",
        "# D√©commentez cette section si vous pr√©f√©rez uploader manuellement\n",
        "# !mkdir -p /content/amalytics-ml/code\n",
        "# print(\"‚úÖ Dossier cr√©√©\")\n",
        "# print(\"\\nüí° Uploadez maintenant votre code via l'interface Files (üìÅ) de Colab\")\n",
        "# print(\"   dans le dossier /content/amalytics-ml/code/\")\n",
        "\n",
        "# Ajouter le code au path Python\n",
        "code_dir = Path(\"/content/amalytics-ml/code\")\n",
        "if not code_dir.exists():\n",
        "    # Si le repo GitHub a une structure diff√©rente, ajustez ici\n",
        "    code_dir = Path(\"/content/amalytics-ml\")\n",
        "\n",
        "src_dir = code_dir / \"src\"\n",
        "if src_dir.exists():\n",
        "    sys.path.insert(0, str(src_dir))\n",
        "    print(f\"‚úÖ Code ajout√© au path: {src_dir}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Dossier src non trouv√© dans {code_dir}\")\n",
        "    print(\"   V√©rifiez la structure de votre repository GitHub\")\n",
        "\n",
        "# V√©rifier la structure\n",
        "print(f\"\\nüìÅ Structure du repository:\")\n",
        "for item in sorted(code_dir.rglob(\"*\"))[:15]:  # Afficher les 15 premiers\n",
        "    if item.is_file() and item.suffix in ['.py', '.json', '.txt', '.md']:\n",
        "        print(f\"  üìÑ {item.relative_to(code_dir)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 5 : V√©rifier les imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V√©rifier que le code peut √™tre import√©\n",
        "try:\n",
        "    from amalytics_ml.config import InferenceConfig\n",
        "    from amalytics_ml.models.inference import run_inference\n",
        "    print(\"‚úÖ Tous les imports fonctionnent!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Erreur d'import: {e}\")\n",
        "    print(\"\\nüí° Assurez-vous que:\")\n",
        "    print(\"   - Le code est bien upload√© dans /content/amalytics-ml/code/\")\n",
        "    print(\"   - La structure est: /content/amalytics-ml/code/src/amalytics_ml/...\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 6 : T√©l√©charger votre mod√®le LoRA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A : Depuis Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ‚ö†Ô∏è MODIFIER LE CHEMIN selon votre structure sur Google Drive\n",
        "LORA_DRIVE_PATH = \"/content/drive/MyDrive/path/to/lora-output\"  # ‚ö†Ô∏è MODIFIER\n",
        "\n",
        "!cp -r {LORA_DRIVE_PATH} /content/lora-output\n",
        "\n",
        "print(\"‚úÖ LoRA copi√© depuis Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B : Depuis Hugging Face Hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# ‚ö†Ô∏è REMPLACER par votre repo Hugging Face\n",
        "LORA_REPO_ID = \"VOTRE_USERNAME/VOTRE_LORA_REPO\"  # ‚ö†Ô∏è MODIFIER\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=LORA_REPO_ID,\n",
        "    local_dir=\"/content/lora-output\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA t√©l√©charg√© depuis Hugging Face\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option C : Upload direct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p /content/lora-output\n",
        "print(\"‚úÖ Dossier cr√©√©\")\n",
        "print(\"\\nüí° Uploadez maintenant vos fichiers LoRA via l'interface Files (üìÅ) de Colab\")\n",
        "print(\"   dans le dossier /content/lora-output/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### V√©rifier que les fichiers LoRA sont pr√©sents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "lora_path = \"/content/lora-output\"\n",
        "if os.path.exists(lora_path):\n",
        "    files = os.listdir(lora_path)\n",
        "    print(f\"‚úÖ {len(files)} fichiers trouv√©s dans {lora_path}\")\n",
        "    print(\"\\nFichiers LoRA:\")\n",
        "    for f in sorted(files)[:10]:\n",
        "        print(f\"  - {f}\")\n",
        "    if len(files) > 10:\n",
        "        print(f\"  ... et {len(files) - 10} autres fichiers\")\n",
        "    \n",
        "    # V√©rifier les fichiers importants\n",
        "    important_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n",
        "    found_important = [f for f in important_files if any(f in file for file in files)]\n",
        "    if found_important:\n",
        "        print(f\"\\n‚úÖ Fichiers LoRA importants trouv√©s: {', '.join(found_important)}\")\n",
        "else:\n",
        "    print(\"‚ùå Dossier LoRA non trouv√©!\")\n",
        "    print(f\"   V√©rifiez que {lora_path} existe\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 7 : Pr√©parer les fichiers de test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p /content/test_data/templates\n",
        "print(\"‚úÖ Dossiers cr√©√©s\")\n",
        "print(\"\\nüìÅ Uploadez maintenant via l'interface Files (üìÅ) de Colab:\")\n",
        "print(\"   - Un PDF de test ‚Üí /content/test_data/\")\n",
        "print(\"   - Un template JSON vide ‚Üí /content/test_data/templates/\")\n",
        "\n",
        "# V√©rifier les fichiers upload√©s\n",
        "import os\n",
        "test_dir = \"/content/test_data\"\n",
        "if os.path.exists(test_dir):\n",
        "    pdfs = [f for f in os.listdir(test_dir) if f.endswith('.pdf')]\n",
        "    templates = []\n",
        "    if os.path.exists(f\"{test_dir}/templates\"):\n",
        "        templates = [f for f in os.listdir(f\"{test_dir}/templates\") if f.endswith('.json')]\n",
        "    \n",
        "    print(f\"\\nüìÑ Fichiers trouv√©s:\")\n",
        "    print(f\"   PDFs: {len(pdfs)} - {pdfs[:3]}\")\n",
        "    print(f\"   Templates: {len(templates)} - {templates[:3]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 8 : Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from amalytics_ml.config import InferenceConfig\n",
        "\n",
        "# ‚ö†Ô∏è MODIFIER LES CHEMINS selon votre structure\n",
        "config = {\n",
        "    \"model_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"lora_path\": \"/content/lora-output\",\n",
        "    \"template_path\": \"/content/test_data/templates/sample_1_template_empty.json\",  # ‚ö†Ô∏è MODIFIER\n",
        "    \"max_new_tokens\": 3000,\n",
        "    \"do_sample\": False,\n",
        "    \"return_scores\": True,\n",
        "    \"load_in_4bit\": True,\n",
        "    \"device_map\": \"auto\",\n",
        "    \"use_batch_inference\": False,\n",
        "    \"apply_anonymization\": False,\n",
        "}\n",
        "\n",
        "cfg = InferenceConfig(**config)\n",
        "\n",
        "# Charger le template\n",
        "if cfg.template_path:\n",
        "    template_path = Path(cfg.template_path)\n",
        "    if template_path.exists():\n",
        "        with template_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            cfg.template = json.load(f)\n",
        "        print(f\"‚úÖ Template charg√©: {cfg.template_path}\")\n",
        "        print(f\"   Cl√©s principales: {list(cfg.template.keys())[:5]}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Template non trouv√©: {cfg.template_path}\")\n",
        "        print(\"   V√©rifiez le chemin dans la configuration ci-dessus\")\n",
        "\n",
        "print(f\"\\n‚úÖ Configuration pr√™te\")\n",
        "print(f\"   Model: {cfg.model_path}\")\n",
        "print(f\"   LoRA: {cfg.lora_path}\")\n",
        "print(f\"   4-bit quantization: {cfg.load_in_4bit}\")\n",
        "print(f\"   Device: {cfg.device_map}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 9 : üöÄ EX√âCUTER L'INF√âRENCE\n",
        "\n",
        "‚ö†Ô∏è **ATTENTION** : Cette √©tape va charger le mod√®le en m√©moire GPU (5-15 minutes la premi√®re fois)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from amalytics_ml.models.inference import run_inference\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "\n",
        "# ‚ö†Ô∏è MODIFIER LE CHEMIN vers votre PDF de test\n",
        "pdf_path = \"/content/test_data/sample_1.pdf\"  # ‚ö†Ô∏è MODIFIER\n",
        "\n",
        "print(\"üöÄ D√©marrage de l'inf√©rence...\")\n",
        "print(\"‚è≥ Chargement du mod√®le (cela peut prendre 5-15 minutes)...\")\n",
        "print(\"üí° Vous pouvez surveiller l'utilisation GPU dans Runtime ‚Üí Manage sessions\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    result = run_inference(\n",
        "        model_path=cfg.model_path,\n",
        "        lora_path=cfg.lora_path if cfg.lora_path else \"\",\n",
        "        input_text=Path(pdf_path),\n",
        "        config=cfg,\n",
        "    )\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    print(\"-\" * 60)\n",
        "    print(f\"‚úÖ Inf√©rence termin√©e avec succ√®s! (temps: {elapsed_time/60:.2f} minutes)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"‚ùå Erreur pendant l'inf√©rence: {e}\")\n",
        "    print(f\"   Temps √©coul√©: {elapsed_time/60:.2f} minutes\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 10 : Afficher les r√©sultats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Afficher le r√©sultat JSON\n",
        "print(\"=\" * 60)\n",
        "print(\"R√âSULTAT JSON\")\n",
        "print(\"=\" * 60)\n",
        "print(json.dumps(result.parsed_json, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Afficher les scores de confiance\n",
        "if result.confidence_scores:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SCORES DE CONFIANCE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(json.dumps(result.confidence_scores, indent=2, ensure_ascii=False))\n",
        "    \n",
        "    # Statistiques\n",
        "    scores = list(result.confidence_scores.values())\n",
        "    print(f\"\\nüìä Statistiques:\")\n",
        "    print(f\"   Nombre de champs: {len(scores)}\")\n",
        "    print(f\"   Score moyen: {sum(scores) / len(scores):.4f}\")\n",
        "    print(f\"   Score min: {min(scores):.4f}\")\n",
        "    print(f\"   Score max: {max(scores):.4f}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Pas de scores de confiance disponibles (return_scores=False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## √âTAPE 11 : Sauvegarder les r√©sultats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = Path(\"/content/outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Sauvegarder le r√©sultat JSON\n",
        "result_file = output_dir / \"result.json\"\n",
        "with result_file.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(result.parsed_json, f, indent=2, ensure_ascii=False)\n",
        "print(f\"‚úÖ R√©sultat sauvegard√©: {result_file}\")\n",
        "\n",
        "# Sauvegarder les scores de confiance si disponibles\n",
        "if result.confidence_scores:\n",
        "    confidence_file = output_dir / \"confidence.json\"\n",
        "    with confidence_file.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result.confidence_scores, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Scores sauvegard√©s: {confidence_file}\")\n",
        "\n",
        "print(f\"\\nüí° Pour t√©l√©charger:\")\n",
        "print(f\"   Clic droit sur les fichiers dans l'interface Files (üìÅ) ‚Üí Download\")\n",
        "print(f\"   Ou utilisez: files.download('{result_file}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualisation des scores (optionnel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if result.confidence_scores:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    scores = list(result.confidence_scores.values())\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(scores, bins=20, edgecolor='black', alpha=0.7)\n",
        "    plt.xlabel('Score de confiance', fontsize=12)\n",
        "    plt.ylabel('Nombre de champs', fontsize=12)\n",
        "    plt.title('Distribution des scores de confiance', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    plt.axvline(avg_score, color='r', linestyle='--', label=f'Moyenne: {avg_score:.3f}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Pas de scores de confiance pour visualiser\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
