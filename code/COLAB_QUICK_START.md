# üöÄ Guide rapide : Inf√©rence LLaMA + LoRA dans Colab

## üìù Checklist rapide

### Pr√©paration (5 minutes)
- [ ] Cr√©er un notebook Colab
- [ ] Activer le GPU (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)
- [ ] Obtenir votre token Hugging Face

### Installation (5-10 minutes)
- [ ] Installer les d√©pendances
- [ ] S'authentifier avec Hugging Face
- [ ] T√©l√©charger/Uploadez votre code
- [ ] T√©l√©charger/Uploadez votre mod√®le LoRA
- [ ] Pr√©parer un PDF de test + template JSON

### Ex√©cution (15-30 minutes)
- [ ] Cr√©er la configuration
- [ ] Lancer l'inf√©rence
- [ ] Voir les r√©sultats

---

## üéØ Script complet (copier-coller)

```python
# ============================================
# √âTAPE 1 : Setup GPU et d√©pendances
# ============================================

import torch
print(f"GPU: {torch.cuda.is_available()}")

!pip install -q torch transformers accelerate bitsandbytes peft pdfplumber pyffx scikit-learn natsort fpdf2 lxml nltk

# ============================================
# √âTAPE 2 : Authentification Hugging Face
# ============================================

from huggingface_hub import login
login("VOTRE_TOKEN_HF")  # ‚ö†Ô∏è REMPLACER

# ============================================
# √âTAPE 3 : Ajouter le code au path
# ============================================

import sys
from pathlib import Path

# Option A : Upload manuel (cr√©er le dossier puis uploader via Files)
!mkdir -p /content/amalytics-ml/code

# Option B : Depuis GitHub
# !git clone https://github.com/VOTRE_REPO/amalytics-ml.git

sys.path.insert(0, "/content/amalytics-ml/code/src")

# ============================================
# √âTAPE 4 : T√©l√©charger le mod√®le LoRA
# ============================================

# Option A : Depuis Google Drive
from google.colab import drive
drive.mount('/content/drive')
!cp -r /content/drive/MyDrive/path/to/lora-output /content/lora-output

# Option B : Depuis Hugging Face
# from huggingface_hub import snapshot_download
# snapshot_download(repo_id="USER/REPO", local_dir="/content/lora-output", token="VOTRE_TOKEN")

# Option C : Upload direct (cr√©er le dossier puis uploader)
# !mkdir -p /content/lora-output

# ============================================
# √âTAPE 5 : Pr√©parer les fichiers de test
# ============================================

!mkdir -p /content/test_data/templates
# Uploadez via l'interface Files:
# - PDF dans /content/test_data/
# - Template JSON dans /content/test_data/templates/

# ============================================
# √âTAPE 6 : Configuration et inf√©rence
# ============================================

import json
from amalytics_ml.config import InferenceConfig
from amalytics_ml.models.inference import run_inference
from pathlib import Path

# Configuration
config = {
    "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "lora_path": "/content/lora-output",
    "template_path": "/content/test_data/templates/sample_1_template_empty.json",  # ‚ö†Ô∏è MODIFIER
    "max_new_tokens": 3000,
    "do_sample": False,
    "return_scores": True,
    "load_in_4bit": True,
    "device_map": "auto",
}

cfg = InferenceConfig(**config)

# Charger le template
with open(cfg.template_path, "r") as f:
    cfg.template = json.load(f)

print("‚úÖ Configuration pr√™te")

# ============================================
# √âTAPE 7 : Lancer l'inf√©rence
# ============================================

pdf_path = "/content/test_data/sample_1.pdf"  # ‚ö†Ô∏è MODIFIER

print("üöÄ Lancement de l'inf√©rence (5-15 minutes)...")

result = run_inference(
    model_path=cfg.model_path,
    lora_path=cfg.lora_path,
    input_text=Path(pdf_path),
    config=cfg,
)

# ============================================
# √âTAPE 8 : Afficher les r√©sultats
# ============================================

print("\n" + "="*60)
print("R√âSULTAT JSON")
print("="*60)
print(json.dumps(result.parsed_json, indent=2, ensure_ascii=False))

if result.confidence_scores:
    print("\n" + "="*60)
    print("SCORES DE CONFIANCE")
    print("="*60)
    print(json.dumps(result.confidence_scores, indent=2, ensure_ascii=False))

# Sauvegarder
output_dir = Path("/content/outputs")
output_dir.mkdir(exist_ok=True)

with (output_dir / "result.json").open("w", encoding="utf-8") as f:
    json.dump(result.parsed_json, f, indent=2, ensure_ascii=False)

print(f"\nüíæ R√©sultat sauvegard√© dans {output_dir}/")
print("‚úÖ Termin√©!")
```

---

## üîß Chemins √† modifier

Dans le script ci-dessus, remplacez :

1. `"VOTRE_TOKEN_HF"` ‚Üí Votre token Hugging Face
2. `"/content/drive/MyDrive/path/to/lora-output"` ‚Üí Chemin vers votre LoRA sur Drive
3. `"/content/test_data/templates/sample_1_template_empty.json"` ‚Üí Chemin vers votre template
4. `"/content/test_data/sample_1.pdf"` ‚Üí Chemin vers votre PDF de test

---

## üìö Documentation compl√®te

Pour plus de d√©tails, consultez : **`COLAB_INFERENCE_GUIDE.md`**

---

## ‚ö†Ô∏è Probl√®mes courants

### Out of Memory
```python
config["load_in_4bit"] = True  # D√©j√† activ√©
config["max_new_tokens"] = 1500  # R√©duire si n√©cessaire
```

### LoRA non trouv√©
```python
import os
print(os.listdir("/content/lora-output"))  # V√©rifier les fichiers
```

### Template non charg√©
```python
from pathlib import Path
template_path = Path("/content/test_data/templates/sample_1_template_empty.json")
print(f"Existe: {template_path.exists()}")
```

---

**Bon test ! üéâ**

