# Guide de configuration pour Google Colab

Ce guide explique comment utiliser le code d'infÃ©rence avec LLaMA + LoRA sur Google Colab.

## âœ… Tests de validation passÃ©s

Tous les tests de structure ont Ã©tÃ© validÃ©s :
- âœ… Imports des modules
- âœ… Configuration
- âœ… Split de template
- âœ… Anonymisation
- âœ… DÃ©tection PDF
- âœ… Chargement de template

## ðŸ“‹ Configuration recommandÃ©e pour Colab

### 1. Configuration JSON pour l'infÃ©rence

CrÃ©ez un fichier `infer_colab.json` :

```json
{
    "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "lora_path": "/content/lora-output",
    "template_path": "/content/data/templates/empty/sample_1_template_empty.json",
    "max_new_tokens": 3000,
    "do_sample": false,
    "return_scores": true,
    "load_in_4bit": true,
    "device_map": "auto",
    "use_batch_inference": true,
    "max_measurements_per_batch": 2,
    "dedup_consecutive_keys": true,
    "apply_anonymization": false,
    "anonymization_use_ner": false,
    "extra_generation_kwargs": {}
}
```

### 2. Configuration avec anonymisation

Si vous voulez activer l'anonymisation :

```json
{
    "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "lora_path": "/content/lora-output",
    "template_path": "/content/data/templates/empty/sample_1_template_empty.json",
    "max_new_tokens": 3000,
    "do_sample": false,
    "return_scores": true,
    "load_in_4bit": true,
    "device_map": "auto",
    "use_batch_inference": true,
    "max_measurements_per_batch": 2,
    "apply_anonymization": true,
    "anonymization_secret_key": "sbh86",
    "anonymization_use_ner": true,
    "extra_generation_kwargs": {}
}
```

## ðŸš€ Code pour Colab

### Setup initial

```python
# Installer les dÃ©pendances
!pip install -q torch transformers accelerate bitsandbytes peft pdfplumber pyffx scikit-learn natsort fpdf2 lxml nltk

# Authentification Hugging Face
from huggingface_hub import login
login("VOTRE_TOKEN_HF")  # Remplacez par votre token

# Importer les modules
import sys
from pathlib import Path

# Ajouter le code au path
sys.path.insert(0, '/content/amalytics-ml/code/src')

from amalytics_ml.config import InferenceConfig
from amalytics_ml.models.inference import run_inference
```

### Exemple d'infÃ©rence simple

```python
# Charger la configuration
with open('/content/infer_colab.json', 'r') as f:
    cfg_dict = json.load(f)

cfg = InferenceConfig(**cfg_dict)

# Charger le template
with open(cfg.template_path, 'r') as f:
    cfg.template = json.load(f)

# ExÃ©cuter l'infÃ©rence
result = run_inference(
    model_path=cfg.model_path,
    lora_path=cfg.lora_path,
    input_text="/content/report.pdf",  # Ou texte directement
    config=cfg,
)

# Afficher les rÃ©sultats
print("RÃ©sultat JSON:")
print(json.dumps(result.parsed_json, indent=2, ensure_ascii=False))

if result.confidence_scores:
    print("\nScores de confiance:")
    print(json.dumps(result.confidence_scores, indent=2, ensure_ascii=False))
```

### Exemple avec anonymisation

```python
# Configuration avec anonymisation activÃ©e
cfg.apply_anonymization = True

# L'infÃ©rence va automatiquement:
# 1. Extraire le texte du PDF
# 2. Anonymiser le texte
# 3. Faire l'infÃ©rence

result = run_inference(
    model_path=cfg.model_path,
    lora_path=cfg.lora_path,
    input_text="/content/report.pdf",
    config=cfg,
)
```

## ðŸ“ Notes importantes

1. **4-bit quantization** : Utilisez `load_in_4bit: true` pour Ã©conomiser la mÃ©moire GPU
2. **Batch inference** : Activez `use_batch_inference: true` pour traiter plus rapidement les grands templates
3. **Anonymisation** : L'anonymisation avec NER peut Ãªtre lente - dÃ©sactivez `anonymization_use_ner` si nÃ©cessaire
4. **Token Hugging Face** : N'oubliez pas de vous authentifier avec votre token HF

## ðŸ”§ DÃ©pannage

### Erreur de mÃ©moire GPU
- RÃ©duisez `max_new_tokens`
- Activez `load_in_4bit: true`
- Utilisez un modÃ¨le plus petit

### Erreur avec LoRA
- VÃ©rifiez que le chemin `lora_path` est correct
- Assurez-vous que les fichiers LoRA sont tÃ©lÃ©chargÃ©s dans Colab

### InfÃ©rence trop lente
- Activez `use_batch_inference: true`
- RÃ©duisez `max_measurements_per_batch`
- Utilisez un GPU plus puissant (T4 -> A100)

## âœ… Checklist avant exÃ©cution

- [ ] Token Hugging Face configurÃ©
- [ ] ModÃ¨le LoRA tÃ©lÃ©chargÃ©
- [ ] Template JSON disponible
- [ ] PDF de test disponible
- [ ] Configuration JSON crÃ©Ã©e
- [ ] GPU activÃ© dans Colab (Runtime -> Change runtime type -> GPU)

