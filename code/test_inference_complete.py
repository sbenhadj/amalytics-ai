"""
Test complet de l'inf√©rence avec toutes les fonctionnalit√©s.

Ce script teste:
1. Extraction de texte depuis PDF
2. Anonymisation (optionnelle)
3. Inf√©rence standard
4. Inf√©rence batch
5. Calcul des scores de confiance

Si tous les tests passent, le code est pr√™t pour Colab avec LLaMA + LoRA.
"""

import json
import sys
from pathlib import Path

# Ajouter src/ au path
ROOT_DIR = Path(__file__).resolve().parent
SRC_DIR = ROOT_DIR / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

from amalytics_ml.config import InferenceConfig
from amalytics_ml.models.inference import run_inference, InferenceResult
from amalytics_ml.data.anonymization import extract_text_from_pdf, anonymize_text, AnonymizationConfig


def test_pdf_extraction():
    """Test l'extraction de texte depuis PDF."""
    print("=" * 70)
    print("TEST 1: Extraction de texte depuis PDF")
    print("=" * 70)
    
    # Chercher un PDF de test
    pdf_path = ROOT_DIR.parent / "amalytics-ml" / "data" / "reports" / "sample_1.pdf"
    
    if not pdf_path.exists():
        # Essayer un autre chemin
        pdf_path = Path("amalytics-ml/data/reports/sample_1.pdf")
        if not pdf_path.exists():
            print(f"‚ö†Ô∏è  PDF de test non trouv√©: {pdf_path}")
            print("   Test ignor√© (n√©cessite un PDF dans amalytics-ml/data/reports/)")
            return True
    
    try:
        text = extract_text_from_pdf(pdf_path)
        print(f"‚úÖ Extraction r√©ussie")
        print(f"   - Fichier: {pdf_path.name}")
        print(f"   - Longueur du texte: {len(text)} caract√®res")
        print(f"   - Aper√ßu (100 premiers caract√®res): {text[:100]}...")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors de l'extraction: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_anonymization():
    """Test l'anonymisation de texte."""
    print("\n" + "=" * 70)
    print("TEST 2: Anonymisation de texte")
    print("=" * 70)
    
    test_text = """
    Patient: Jean Dupont
    Date de naissance: 15/03/1985
    Email: jean.dupont@example.com
    T√©l√©phone: 0612345678
    Code postal: 75001 PARIS
    """
    
    try:
        config = AnonymizationConfig(
            secret_key=b"test_key",
            use_ner=False,  # D√©sactiver NER pour test rapide
            anonymize_codes=True,
            anonymize_dates=True,
            anonymize_emails=True,
            anonymize_phones=True,
            anonymize_postal_codes=True,
        )
        
        anonymized = anonymize_text(test_text, config)
        print(f"‚úÖ Anonymisation r√©ussie")
        print(f"   - Texte original: {len(test_text)} caract√®res")
        print(f"   - Texte anonymis√©: {len(anonymized)} caract√®res")
        print(f"   - Texte modifi√©: {'Oui' if test_text != anonymized else 'Non'}")
        print(f"   - Aper√ßu anonymis√©: {anonymized[:150]}...")
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors de l'anonymisation: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_config_loading():
    """Test le chargement de configuration."""
    print("\n" + "=" * 70)
    print("TEST 3: Chargement de configuration")
    print("=" * 70)
    
    try:
        # Test avec configuration minimale
        config = InferenceConfig(
            model_path="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            lora_path="",
            template_path="amalytics-ml/data/filtered_templates/empty/sample_1_template_empty.json",
            apply_anonymization=False,
        )
        
        print(f"‚úÖ Configuration cr√©√©e")
        print(f"   - apply_anonymization: {config.apply_anonymization}")
        print(f"   - use_batch_inference: {config.use_batch_inference}")
        
        # Test avec anonymisation activ√©e
        config_anon = InferenceConfig(
            model_path="test/model",
            lora_path="test/lora",
            apply_anonymization=True,
            anonymization_use_ner=False,
        )
        
        print(f"‚úÖ Configuration avec anonymisation cr√©√©e")
        print(f"   - apply_anonymization: {config_anon.apply_anonymization}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erreur de configuration: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_template_loading():
    """Test le chargement de template."""
    print("\n" + "=" * 70)
    print("TEST 4: Chargement de template")
    print("=" * 70)
    
    template_path = ROOT_DIR.parent / "amalytics-ml" / "data" / "filtered_templates" / "empty" / "sample_1_template_empty.json"
    
    if not template_path.exists():
        template_path = Path("amalytics-ml/data/filtered_templates/empty/sample_1_template_empty.json")
        if not template_path.exists():
            print(f"‚ö†Ô∏è  Template non trouv√©: {template_path}")
            print("   Test ignor√©")
            return True
    
    try:
        with template_path.open("r", encoding="utf-8") as f:
            template = json.load(f)
        
        print(f"‚úÖ Template charg√©")
        print(f"   - Fichier: {template_path.name}")
        print(f"   - Cl√©s de premier niveau: {list(template.keys())[:3]}")
        
        # Compter les mesures
        def count_measurements(obj, count=0):
            if isinstance(obj, dict):
                if "valeur" in obj:
                    return count + 1
                return sum(count_measurements(v, count) for v in obj.values())
            return count
        
        measurements = count_measurements(template)
        print(f"   - Nombre de mesures (champs avec 'valeur'): {measurements}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_inference_without_model():
    """Test la structure d'inf√©rence sans charger le mod√®le (test rapide)."""
    print("\n" + "=" * 70)
    print("TEST 5: Structure d'inf√©rence (sans mod√®le)")
    print("=" * 70)
    
    try:
        # Cr√©er un template minimal
        test_template = {
            "Hematologie": {
                "NumerationGlobulaire": {
                    "Hematies": {"valeur": None, "unit√©": "T/L"}
                }
            }
        }
        
        config = InferenceConfig(
            model_path="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            lora_path="",
            template=test_template,
            max_new_tokens=50,  # Tr√®s court pour test rapide
            return_scores=False,
            apply_anonymization=False,
            use_batch_inference=False,
        )
        
        print(f"‚úÖ Configuration d'inf√©rence cr√©√©e")
        print(f"   - Template int√©gr√©: Oui")
        print(f"   - Anonymisation: {'Activ√©e' if config.apply_anonymization else 'D√©sactiv√©e'}")
        print(f"   - Batch inference: {'Activ√©' if config.use_batch_inference else 'D√©sactiv√©'}")
        
        # Test avec PDF path
        pdf_path = ROOT_DIR.parent / "amalytics-ml" / "data" / "reports" / "sample_1.pdf"
        if pdf_path.exists():
            print(f"‚úÖ PDF d√©tect√©: {pdf_path.name}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_batch_inference_config():
    """Test la configuration pour batch inference."""
    print("\n" + "=" * 70)
    print("TEST 6: Configuration batch inference")
    print("=" * 70)
    
    try:
        config = InferenceConfig(
            model_path="test/model",
            lora_path="test/lora",
            use_batch_inference=True,
            max_measurements_per_batch=3,
            dedup_consecutive_keys=True,
        )
        
        print(f"‚úÖ Configuration batch cr√©√©e")
        print(f"   - use_batch_inference: {config.use_batch_inference}")
        print(f"   - max_measurements_per_batch: {config.max_measurements_per_batch}")
        print(f"   - dedup_consecutive_keys: {config.dedup_consecutive_keys}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False


def print_summary(results: dict[str, bool]):
    """Affiche le r√©sum√© des tests."""
    print("\n" + "=" * 70)
    print("R√âSUM√â DES TESTS")
    print("=" * 70)
    
    total = len(results)
    passed = sum(1 for v in results.values() if v)
    failed = total - passed
    
    for name, result in results.items():
        status = "‚úÖ PASS√â" if result else "‚ùå √âCHOU√â"
        print(f"{name:.<50} {status}")
    
    print(f"\nTotal: {total} tests")
    print(f"R√©ussis: {passed} ‚úÖ")
    print(f"√âchou√©s: {failed} ‚ùå")
    
    if passed == total:
        print("\n" + "üéâ" * 35)
        print("TOUS LES TESTS SONT PASS√âS!")
        print("Le code est pr√™t pour Colab avec LLaMA + LoRA.")
        print("üéâ" * 35)
        print("\nProchaines √©tapes pour Colab:")
        print("1. T√©l√©charger le mod√®le LoRA fine-tun√©")
        print("2. Configurer InferenceConfig avec:")
        print("   - model_path: 'meta-llama/Meta-Llama-3.1-8B-Instruct'")
        print("   - lora_path: chemin vers votre LoRA")
        print("   - apply_anonymization: True (si n√©cessaire)")
        print("   - use_batch_inference: True (pour optimisation)")
        return True
    else:
        print("\n‚ö†Ô∏è  Certains tests ont √©chou√©.")
        print("   Corrigez les erreurs avant de passer √† Colab.")
        return False


def main():
    """Ex√©cute tous les tests."""
    print("\n" + "üß™" * 35)
    print("TESTS COMPLETS DE L'INF√âRENCE")
    print("üß™" * 35)
    print("\nCe script teste toutes les fonctionnalit√©s avant de passer √† Colab.\n")
    
    results = {}
    
    # Tests (dans l'ordre de d√©pendance)
    results["Extraction PDF"] = test_pdf_extraction()
    results["Anonymisation"] = test_anonymization()
    results["Configuration"] = test_config_loading()
    results["Chargement template"] = test_template_loading()
    results["Structure inf√©rence"] = test_inference_without_model()
    results["Configuration batch"] = test_batch_inference_config()
    
    # R√©sum√©
    all_passed = print_summary(results)
    
    sys.exit(0 if all_passed else 1)


if __name__ == "__main__":
    main()

