# Guide complet : Tester l'inf√©rence LLaMA + LoRA dans Google Colab

Ce guide vous explique √©tape par √©tape comment tester votre mod√®le LLaMA fine-tun√© avec LoRA dans Google Colab.

---

## üìã Pr√©requis

- Un compte Google avec acc√®s √† [Google Colab](https://colab.research.google.com/)
- Un token Hugging Face avec acc√®s au mod√®le LLaMA (si le mod√®le est gated)
- Votre mod√®le LoRA fine-tun√© (fichiers LoRA sauvegard√©s)

---

## üöÄ √âTAPE 1 : Pr√©parer Colab

### 1.1 Cr√©er un nouveau notebook Colab

1. Allez sur [Google Colab](https://colab.research.google.com/)
2. Cliquez sur **"New notebook"**
3. Renommez le notebook (par exemple : "Test Inference LLaMA LoRA")

### 1.2 Activer le GPU

1. Dans le menu, cliquez sur **Runtime** ‚Üí **Change runtime type**
2. S√©lectionnez **T4 GPU** (gratuit) ou **A100** (payant, plus rapide)
3. Cliquez sur **Save**

### 1.3 V√©rifier que le GPU est actif

Ex√©cutez cette cellule pour v√©rifier :

```python
import torch
print(f"GPU disponible: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
```

---

## üì¶ √âTAPE 2 : Installer les d√©pendances

Ex√©cutez cette cellule pour installer tous les packages n√©cessaires :

```python
!pip install -q torch transformers accelerate bitsandbytes peft pdfplumber pyffx scikit-learn natsort fpdf2 lxml nltk
```

‚ö†Ô∏è **Note** : L'installation prend quelques minutes. Attendez que ce soit termin√© avant de continuer.

---

## üîê √âTAPE 3 : Authentification Hugging Face

### 3.1 Obtenir votre token Hugging Face

1. Allez sur [huggingface.co](https://huggingface.co/)
2. Connectez-vous ou cr√©ez un compte
3. Allez dans **Settings** ‚Üí **Access Tokens**
4. Cr√©ez un nouveau token (ou utilisez un token existant)
5. Copiez le token (commence par `hf_...`)

### 3.2 Se connecter dans Colab

```python
from huggingface_hub import login

# Remplacez VOTRE_TOKEN par votre vrai token Hugging Face
login("VOTRE_TOKEN_HF")
```

Ex√©cutez cette cellule. Un lien appara√Ætra - cliquez dessus pour autoriser l'acc√®s, puis revenez √† Colab.

---

## üìÅ √âTAPE 4 : T√©l√©charger votre code et vos fichiers

### 4.1 Option A : Depuis GitHub (si votre code est sur GitHub)

```python
!git clone https://github.com/VOTRE_REPO/amalytics-ml.git
```

### 4.2 Option B : Upload manuel dans Colab

1. Cr√©ez un dossier pour votre code :
```python
!mkdir -p /content/amalytics-ml/code
```

2. Cliquez sur l'ic√¥ne üìÅ (Files) dans la barre lat√©rale de Colab
3. Uploadez votre code dans `/content/amalytics-ml/code/`

### 4.3 Ajouter le code au Python path

```python
import sys
from pathlib import Path

# Ajouter le dossier src au path Python
code_dir = Path("/content/amalytics-ml/code")
src_dir = code_dir / "src"
sys.path.insert(0, str(src_dir))

print(f"‚úÖ Code ajout√© au path: {src_dir}")
```

---

## üì• √âTAPE 5 : T√©l√©charger votre mod√®le LoRA

Vous devez avoir vos fichiers LoRA quelque part (Google Drive, Hugging Face Hub, etc.).

### 5.1 Option A : Depuis Google Drive

```python
from google.colab import drive
drive.mount('/content/drive')

# Copier depuis Google Drive vers Colab
!cp -r /content/drive/MyDrive/path/to/your/lora-output /content/lora-output
```

### 5.2 Option B : Depuis Hugging Face Hub

Si votre LoRA est sur Hugging Face :

```python
from huggingface_hub import snapshot_download

# Remplacez par votre repo Hugging Face
snapshot_download(
    repo_id="VOTRE_USERNAME/VOTRE_LORA_REPO",
    local_dir="/content/lora-output",
    token="VOTRE_TOKEN_HF"  # Optionnel si d√©j√† connect√©
)
```

### 5.3 Option C : Upload direct dans Colab

```python
!mkdir -p /content/lora-output
```

Puis uploadez vos fichiers LoRA via l'interface Files de Colab.

### 5.4 V√©rifier que les fichiers LoRA sont pr√©sents

```python
import os
lora_path = "/content/lora-output"
if os.path.exists(lora_path):
    files = os.listdir(lora_path)
    print(f"‚úÖ Fichiers LoRA trouv√©s ({len(files)} fichiers):")
    for f in files[:10]:  # Afficher les 10 premiers
        print(f"  - {f}")
    if len(files) > 10:
        print(f"  ... et {len(files) - 10} autres fichiers")
else:
    print("‚ùå Dossier LoRA non trouv√©!")
```

---

## üìÑ √âTAPE 6 : Pr√©parer les fichiers de test

### 6.1 T√©l√©charger ou uploader un PDF de test

```python
!mkdir -p /content/test_data
```

Puis uploadez un PDF de test dans `/content/test_data/` via l'interface Files.

### 6.2 T√©l√©charger ou cr√©er un template JSON vide

```python
!mkdir -p /content/test_data/templates
```

Uploadez votre template JSON vide dans `/content/test_data/templates/`.

### 6.3 V√©rifier les fichiers

```python
import os
test_dir = "/content/test_data"
if os.path.exists(test_dir):
    print("üìÅ Fichiers de test:")
    for root, dirs, files in os.walk(test_dir):
        level = root.replace(test_dir, '').count(os.sep)
        indent = ' ' * 2 * level
        print(f'{indent}{os.path.basename(root)}/')
        subindent = ' ' * 2 * (level + 1)
        for file in files[:5]:  # Limiter l'affichage
            print(f'{subindent}{file}')
```

---

## ‚öôÔ∏è √âTAPE 7 : Cr√©er la configuration d'inf√©rence

Cr√©ez un fichier de configuration JSON. Vous pouvez le cr√©er directement dans Colab :

```python
import json

config = {
    "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "lora_path": "/content/lora-output",
    "template_path": "/content/test_data/templates/sample_1_template_empty.json",
    "max_new_tokens": 3000,
    "do_sample": False,
    "return_scores": True,
    "load_in_4bit": True,
    "device_map": "auto",
    "use_batch_inference": False,
    "max_measurements_per_batch": 2,
    "dedup_consecutive_keys": True,
    "apply_anonymization": False,
    "extra_generation_kwargs": {}
}

# Sauvegarder la configuration
with open("/content/infer_config.json", "w") as f:
    json.dump(config, f, indent=2)

print("‚úÖ Configuration cr√©√©e: /content/infer_config.json")
print(json.dumps(config, indent=2))
```

---

## üîç √âTAPE 8 : Tester les imports

V√©rifiez que tous les modules peuvent √™tre import√©s :

```python
try:
    from amalytics_ml.config import InferenceConfig
    from amalytics_ml.models.inference import run_inference
    print("‚úÖ Imports r√©ussis!")
except Exception as e:
    print(f"‚ùå Erreur d'import: {e}")
    import traceback
    traceback.print_exc()
```

---

## üéØ √âTAPE 9 : Ex√©cuter l'inf√©rence

### 9.1 Charger la configuration

```python
import json
from pathlib import Path
from amalytics_ml.config import InferenceConfig

# Charger la config
with open("/content/infer_config.json", "r") as f:
    cfg_dict = json.load(f)

# Cr√©er l'objet InferenceConfig
cfg = InferenceConfig(**cfg_dict)

# Charger le template si n√©cessaire
if cfg.template is None and cfg.template_path:
    template_path = Path(cfg.template_path)
    if template_path.exists():
        with template_path.open("r", encoding="utf-8") as f:
            cfg.template = json.load(f)
        print(f"‚úÖ Template charg√©: {cfg.template_path}")
    else:
        print(f"‚ùå Template non trouv√©: {cfg.template_path}")
```

### 9.2 V√©rifier les chemins

```python
print("üìÅ V√©rification des chemins:")
print(f"  Model: {cfg.model_path}")
print(f"  LoRA: {cfg.lora_path}")
print(f"  Template: {cfg.template_path if cfg.template_path else 'Inclus dans config'}")

# V√©rifier l'existence des fichiers locaux
from pathlib import Path

if cfg.lora_path:
    lora_path = Path(cfg.lora_path)
    if lora_path.exists():
        print(f"  ‚úÖ LoRA trouv√©: {len(list(lora_path.glob('*')))} fichiers")
    else:
        print(f"  ‚ö†Ô∏è  LoRA non trouv√© (sera t√©l√©charg√© si HuggingFace ID)")

if cfg.template:
    print(f"  ‚úÖ Template charg√© en m√©moire")
```

### 9.3 Ex√©cuter l'inf√©rence

‚ö†Ô∏è **ATTENTION** : Cette √©tape va charger le mod√®le complet en m√©moire GPU. Cela peut prendre 5-15 minutes.

```python
from amalytics_ml.models.inference import run_inference
from pathlib import Path
import json

# Chemin vers votre PDF de test
pdf_path = "/content/test_data/sample_1.pdf"  # Remplacez par votre PDF

print("üöÄ D√©marrage de l'inf√©rence...")
print("‚è≥ Chargement du mod√®le (cela peut prendre plusieurs minutes)...")

try:
    # Ex√©cuter l'inf√©rence
    result = run_inference(
        model_path=cfg.model_path,
        lora_path=cfg.lora_path if cfg.lora_path else "",
        input_text=Path(pdf_path),  # Le code d√©tectera automatiquement que c'est un PDF
        config=cfg,
    )
    
    print("‚úÖ Inf√©rence termin√©e!")
    
    # Afficher le r√©sultat
    print("\n" + "="*60)
    print("R√âSULTAT JSON")
    print("="*60)
    print(json.dumps(result.parsed_json, indent=2, ensure_ascii=False))
    
    # Afficher les scores de confiance si disponibles
    if result.confidence_scores:
        print("\n" + "="*60)
        print("SCORES DE CONFIANCE")
        print("="*60)
        print(json.dumps(result.confidence_scores, indent=2, ensure_ascii=False))
    
    # Sauvegarder les r√©sultats
    output_dir = Path("/content/outputs")
    output_dir.mkdir(exist_ok=True)
    
    with (output_dir / "result.json").open("w", encoding="utf-8") as f:
        json.dump(result.parsed_json, f, indent=2, ensure_ascii=False)
    
    if result.confidence_scores:
        with (output_dir / "confidence.json").open("w", encoding="utf-8") as f:
            json.dump(result.confidence_scores, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ R√©sultats sauvegard√©s dans {output_dir}/")
    
except Exception as e:
    print(f"‚ùå Erreur pendant l'inf√©rence: {e}")
    import traceback
    traceback.print_exc()
```

---

## üìä √âTAPE 10 : Analyser les r√©sultats

### 10.1 Comparer avec la ground truth (si disponible)

```python
# Si vous avez une ground truth
gt_path = "/content/test_data/ground_truth/sample_1.json"
if Path(gt_path).exists():
    with open(gt_path, "r") as f:
        ground_truth = json.load(f)
    
    print("Comparaison avec la ground truth:")
    # Ici vous pouvez ajouter votre logique de comparaison
```

### 10.2 Visualiser les scores de confiance

```python
if result.confidence_scores:
    import matplotlib.pyplot as plt
    
    scores = list(result.confidence_scores.values())
    
    plt.figure(figsize=(10, 6))
    plt.hist(scores, bins=20, edgecolor='black')
    plt.xlabel('Score de confiance')
    plt.ylabel('Nombre de champs')
    plt.title('Distribution des scores de confiance')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    print(f"Score moyen: {sum(scores) / len(scores):.4f}")
    print(f"Score min: {min(scores):.4f}")
    print(f"Score max: {max(scores):.4f}")
```

---

## üîß √âTAPE 11 : D√©pannage

### Probl√®me : Out of Memory (OOM)

**Solution 1** : R√©duire `max_new_tokens`
```python
config["max_new_tokens"] = 1500  # Au lieu de 3000
```

**Solution 2** : S'assurer que 4-bit est activ√©
```python
config["load_in_4bit"] = True
```

**Solution 3** : Utiliser un GPU plus puissant (A100 au lieu de T4)

### Probl√®me : Erreur lors du chargement du mod√®le

```python
# V√©rifier la version de transformers
!pip show transformers

# Mettre √† jour si n√©cessaire
!pip install -U transformers
```

### Probl√®me : LoRA non trouv√©

```python
# V√©rifier le contenu du dossier LoRA
import os
lora_path = "/content/lora-output"
print("Contenu du dossier LoRA:")
for f in os.listdir(lora_path):
    print(f"  - {f}")
```

### Probl√®me : Template non charg√©

```python
# V√©rifier que le template est bien charg√©
if cfg.template:
    print(f"‚úÖ Template charg√©: {type(cfg.template)}")
    print(f"Cl√©s principales: {list(cfg.template.keys())[:5]}")
else:
    print("‚ùå Template non charg√©")
```

---

## ‚úÖ Checklist finale

Avant de lancer l'inf√©rence, v√©rifiez :

- [ ] GPU activ√© dans Colab (Runtime ‚Üí Change runtime type)
- [ ] Toutes les d√©pendances install√©es
- [ ] Authentification Hugging Face r√©ussie
- [ ] Code t√©l√©charg√©/upload√© dans Colab
- [ ] Fichiers LoRA t√©l√©charg√©s/upload√©s
- [ ] PDF de test disponible
- [ ] Template JSON disponible
- [ ] Configuration JSON cr√©√©e

---

## üéâ Exemple complet (tout-en-un)

Voici un script complet que vous pouvez ex√©cuter dans une seule cellule (apr√®s avoir t√©l√©charg√© vos fichiers) :

```python
# ============================================
# INF√âRENCE COMPL√àTE LLaMA + LoRA dans Colab
# ============================================

import json
import sys
from pathlib import Path

# 1. Setup
print("üì¶ Installation des d√©pendances...")
!pip install -q torch transformers accelerate bitsandbytes peft pdfplumber pyffx scikit-learn natsort fpdf2 lxml nltk

# 2. Authentification
print("üîê Authentification Hugging Face...")
from huggingface_hub import login
login("VOTRE_TOKEN_HF")  # ‚ö†Ô∏è REMPLACER PAR VOTRE TOKEN

# 3. Ajouter le code au path
code_dir = Path("/content/amalytics-ml/code")
src_dir = code_dir / "src"
sys.path.insert(0, str(src_dir))
print(f"‚úÖ Code ajout√© au path: {src_dir}")

# 4. Configuration
print("‚öôÔ∏è Configuration...")
config = {
    "model_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "lora_path": "/content/lora-output",
    "template_path": "/content/test_data/templates/sample_1_template_empty.json",
    "max_new_tokens": 3000,
    "do_sample": False,
    "return_scores": True,
    "load_in_4bit": True,
    "device_map": "auto",
}

cfg = InferenceConfig(**config)

# Charger le template
with open(cfg.template_path, "r") as f:
    cfg.template = json.load(f)
print("‚úÖ Configuration pr√™te")

# 5. Inf√©rence
print("üöÄ Lancement de l'inf√©rence...")
from amalytics_ml.models.inference import run_inference

result = run_inference(
    model_path=cfg.model_path,
    lora_path=cfg.lora_path,
    input_text="/content/test_data/sample_1.pdf",
    config=cfg,
)

# 6. R√©sultats
print("\n" + "="*60)
print("R√âSULTAT")
print("="*60)
print(json.dumps(result.parsed_json, indent=2, ensure_ascii=False))

print("\n‚úÖ Inf√©rence termin√©e avec succ√®s!")
```

---

## üìù Notes importantes

1. **Temps d'ex√©cution** : Le chargement du mod√®le peut prendre 5-15 minutes la premi√®re fois
2. **M√©moire GPU** : Utilisez `load_in_4bit: true` pour √©conomiser la m√©moire
3. **Token Hugging Face** : N√©cessaire si le mod√®le est "gated" (acc√®s restreint)
4. **Sauvegarde** : Les fichiers dans Colab sont temporaires - t√©l√©chargez les r√©sultats avant de fermer !

---

## üÜò Besoin d'aide ?

Si vous rencontrez des probl√®mes :

1. V√©rifiez les messages d'erreur dans Colab
2. Consultez la section "D√©pannage" ci-dessus
3. V√©rifiez que tous les chemins de fichiers sont corrects
4. Assurez-vous que le GPU est bien activ√©

---

**Bon test ! üöÄ**

