{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNc+n5h/0dVaOADMkElKrDY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvsUju5SEShx","executionInfo":{"status":"ok","timestamp":1755384551283,"user_tz":-120,"elapsed":97,"user":{"displayName":"Saima Ben Hadj","userId":"02052565308082602743"}},"outputId":"9b34d791-d9f0-4e68-c224-5aadc7601c9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["3 parties générées.\n"]}],"source":["import json\n","import os\n","from pathlib import Path\n","import ast\n","\n","def _detect_format(file_path: str) -> str:\n","    \"\"\"\n","    Détecte le format du fichier : 'array', 'object' ou 'jsonl'.\n","    \"\"\"\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        # Lire un peu de contenu pour détecter rapidement\n","        head = f.read(4096)\n","        if not head:\n","            raise ValueError(\"Fichier vide.\")\n","\n","        # Détection JSON classique\n","        first = head.lstrip()[:1]\n","        if first in (\"[\", \"{\"):\n","            try:\n","                _ = json.loads(head)  # si tout petit fichier\n","                return \"array\" if first == \"[\" else \"object\"\n","            except json.JSONDecodeError:\n","                # Si gros fichier, recharge tout pour trancher\n","                f.seek(0)\n","                data = json.load(f)\n","                return \"array\" if isinstance(data, list) else \"object\"\n","\n","        # Sinon JSONL probable : vérifier première ligne non vide\n","        f.seek(0)\n","        for line in f:\n","            s = line.strip()\n","            if not s:\n","                continue\n","            try:\n","                json.loads(s)\n","                return \"jsonl\"\n","            except json.JSONDecodeError:\n","                # Parfois l'IA renvoie dict style Python -> tenter ast.literal_eval\n","                try:\n","                    ast.literal_eval(s)\n","                    return \"jsonl\"\n","                except Exception:\n","                    pass\n","                break\n","\n","    # Si on arrive ici, on tente un dernier recours : charger entièrement\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        try:\n","            data = json.load(f)\n","            return \"array\" if isinstance(data, list) else \"object\"\n","        except json.JSONDecodeError:\n","            return \"jsonl\"\n","\n","def _even_splits(total: int, n: int):\n","    \"\"\"\n","    Retourne une liste de tailles de segments qui se répartissent au mieux total éléments sur n parts.\n","    Exemple: total=10, n=3 -> [4,3,3]\n","    \"\"\"\n","    if n <= 0:\n","        raise ValueError(\"n doit être > 0\")\n","    base, rem = divmod(total, n)\n","    return [(base + 1 if i < rem else base) for i in range(n)]\n","\n","def split_json_file(\n","    file_path: str,\n","    n: int,\n","    output_dir: str | None = None,\n","    keep_jsonl_format: bool = True,\n",") -> list[str]:\n","    \"\"\"\n","    Coupe un fichier JSON en N sous-fichiers et renvoie une liste de textes JSON.\n","    - Gère tableau JSON, objet JSON et JSONL.\n","    - Si output_dir est fourni, écrit aussi les sous-fichiers sur disque.\n","    - Pour JSONL, si keep_jsonl_format=True : sorties au format JSONL.\n","      Sinon, chaque sortie est un tableau JSON (liste d'objets).\n","\n","    Returns:\n","        List[str]: liste des contenus (texte) des N parties.\n","    \"\"\"\n","    fmt = _detect_format(file_path)\n","    p = Path(file_path)\n","    if output_dir:\n","        Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","    outputs: list[str] = []\n","\n","    if fmt == \"array\":\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        if not isinstance(data, list):\n","            raise ValueError(\"Format détecté 'array' mais contenu non-liste.\")\n","\n","        sizes = _even_splits(len(data), n)\n","        idx = 0\n","        for part_id, sz in enumerate(sizes, start=1):\n","            chunk = data[idx: idx + sz]\n","            idx += sz\n","            text = json.dumps(chunk, ensure_ascii=False, indent=2)\n","            outputs.append(text)\n","\n","            if output_dir is not None:\n","                out_path = Path(output_dir) / f\"{p.stem}_part{part_id}.json\"\n","                out_path.write_text(text, encoding=\"utf-8\")\n","\n","        # Si certaines parts sont vides (possible quand n > len(data))\n","        for part_id in range(len(sizes) + 1, n + 1):\n","            text = \"[]\"\n","            outputs.append(text)\n","            if output_dir is not None:\n","                out_path = Path(output_dir) / f\"{p.stem}_part{part_id}.json\"\n","                out_path.write_text(text, encoding=\"utf-8\")\n","\n","        return outputs\n","\n","    if fmt == \"object\":\n","        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        if not isinstance(data, dict):\n","            raise ValueError(\"Format détecté 'object' mais contenu non-dict.\")\n","\n","        keys = list(data.keys())\n","        sizes = _even_splits(len(keys), n)\n","        idx = 0\n","        for part_id, sz in enumerate(sizes, start=1):\n","            part_keys = keys[idx: idx + sz]\n","            idx += sz\n","            chunk = {k: data[k] for k in part_keys}\n","            text = json.dumps(chunk, ensure_ascii=False, indent=2)\n","            outputs.append(text)\n","\n","            if output_dir is not None:\n","                out_path = Path(output_dir) / f\"{p.stem}_part{part_id}.json\"\n","                out_path.write_text(text, encoding=\"utf-8\")\n","\n","        # Parts vides si n > nb de clés\n","        for part_id in range(len(sizes) + 1, n + 1):\n","            text = \"{}\"\n","            outputs.append(text)\n","            if output_dir is not None:\n","                out_path = Path(output_dir) / f\"{p.stem}_part{part_id}.json\"\n","                out_path.write_text(text, encoding=\"utf-8\")\n","\n","        return outputs\n","\n","    # JSONL\n","    # On lit les lignes non vides et on vérifie qu'elles sont des JSON valides ou des dict Python.\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        rows = []\n","        for line in f:\n","            s = line.strip()\n","            if not s:\n","                continue\n","            # Normaliser : si ce n'est pas du JSON strict, on tente ast.literal_eval puis dump JSON\n","            try:\n","                _ = json.loads(s)\n","                rows.append(s)\n","            except json.JSONDecodeError:\n","                # tenter dict/list style Python\n","                try:\n","                    obj = ast.literal_eval(s)\n","                    rows.append(json.dumps(obj, ensure_ascii=False))\n","                except Exception:\n","                    # si une ligne n'est pas parseable, on peut choisir de la sauter ou lever\n","                    raise ValueError(f\"Ligne JSONL non parseable:\\n{s}\")\n","\n","    sizes = _even_splits(len(rows), n)\n","    idx = 0\n","    for part_id, sz in enumerate(sizes, start=1):\n","        chunk_rows = rows[idx: idx + sz]\n","        idx += sz\n","\n","        if keep_jsonl_format:\n","            text = \"\\n\".join(chunk_rows) + (\"\\n\" if chunk_rows else \"\")\n","            ext = \".jsonl\"\n","        else:\n","            # Réémettre chaque part comme un tableau JSON\n","            objs = [json.loads(r) for r in chunk_rows]\n","            text = json.dumps(objs, ensure_ascii=False, indent=2)\n","            ext = \".json\"\n","\n","        outputs.append(text)\n","\n","        if output_dir is not None:\n","            out_path = Path(output_dir) / f\"{p.stem}_part{part_id}{ext}\"\n","            out_path.write_text(text, encoding=\"utf-8\")\n","\n","    # Parts vides si n > nb de lignes\n","    for part_id in range(len(sizes) + 1, n + 1):\n","        if keep_jsonl_format:\n","            text, ext = \"\", \".jsonl\"\n","        else:\n","            text, ext = \"[]\", \".json\"\n","        outputs.append(text)\n","        if output_dir is not None:\n","            out_path = Path(output_dir) / f\"{p.stem}_part{part_id}{ext}\"\n","            out_path.write_text(text, encoding=\"utf-8\")\n","\n","    return outputs\n","\n","\n","# ----------- Exemple d'utilisation -------------\n","if __name__ == \"__main__\":\n","    # Couper en 3 parties et écrire les fichiers dans ./out\n","    parts_text = split_json_file(\"/content/sample_data/52_annotation_empty.json\", n=3, output_dir=\"out\", keep_jsonl_format=True)\n","    print(f\"{len(parts_text)} parties générées.\")\n","    # parts_text est une liste de chaînes JSON/JSONL prêtes à l'usage.\n"]},{"cell_type":"code","source":["import json\n","from pathlib import Path\n","from typing import Any, Dict, List, Tuple, Union\n","\n","JsonDict = Dict[str, Any]\n","PathT = Tuple[str, ...]\n","\n","\n","def is_measurement_node(node: Any) -> bool:\n","    \"\"\"\n","    Un 'objet' (feuille) est un dict qui contient au moins la clé 'valeur'.\n","    (On n'impose pas 'unité' pour couvrir les cas qui n'en ont pas.)\n","    \"\"\"\n","    return isinstance(node, dict) and \"valeur\" in node and \"unité\" in node\n","\n","\n","\n","def normalize_consecutive_duplicates(path: PathT) -> PathT:\n","    \"\"\"Supprime les doublons consécutifs dans le chemin ('A','A','B' -> 'A','B').\"\"\"\n","    if not path:\n","        return path\n","    norm = [path[0]]\n","    for k in path[1:]:\n","        if k != norm[-1]:\n","            norm.append(k)\n","    return tuple(norm)\n","\n","\n","def collect_measurements(node: Any, base_path: PathT = ()) -> List[Tuple[PathT, JsonDict]]:\n","    \"\"\"\n","    Parcourt récursivement un JSON et renvoie la liste [(path, leaf_dict), ...]\n","    où leaf_dict est un objet-mesure (dict avec 'valeur').\n","    \"\"\"\n","    out: List[Tuple[PathT, JsonDict]] = []\n","\n","    if is_measurement_node(node):\n","        out.append((base_path, node))  # la feuille est ici\n","        return out\n","\n","    if isinstance(node, dict):\n","        for k, v in node.items():\n","            out.extend(collect_measurements(v, base_path + (k,)))\n","    elif isinstance(node, list):\n","        # si jamais il y a des listes, on indexe (ex: ...,\"items\",\"[0]\", ...)\n","        for i, v in enumerate(node):\n","            out.extend(collect_measurements(v, base_path + (f\"[{i}]\",)))\n","    # autres types: rien à faire\n","    return out\n","\n","\n","def insert_path(root: JsonDict, path: PathT, value: JsonDict, dedup: bool = True) -> None:\n","    \"\"\"Insère value dans root au chemin path (en créant les sous-dicts).\"\"\"\n","    if dedup:\n","        path = normalize_consecutive_duplicates(path)\n","    if not path:\n","        # cas extrême: le noeud racine est lui-même une feuille (peu probable)\n","        root.update(value)\n","        return\n","\n","    cur = root\n","    for key in path[:-1]:\n","        if key not in cur or not isinstance(cur[key], dict):\n","            cur[key] = {}\n","        cur = cur[key]\n","    cur[path[-1]] = value\n","\n","\n","def split_dict_by_measurements(\n","    data: JsonDict,\n","    max_objects_per_part: int,\n","    dedup_consecutive: bool = True,\n",") -> List[JsonDict]:\n","    \"\"\"\n","    Découpe un dict JSON en parties de <= max_objects_per_part 'feuilles' (objets-mesures).\n","    Chaque partie contient uniquement les branches nécessaires pour atteindre ses feuilles.\n","    \"\"\"\n","    if max_objects_per_part <= 0:\n","        raise ValueError(\"max_objects_per_part doit être > 0\")\n","\n","    leaves = collect_measurements(data)\n","    if not leaves:\n","        # Rien à découper : retourne une partie vide\n","        return [{}]\n","\n","    parts: List[JsonDict] = []\n","    for i in range(0, len(leaves), max_objects_per_part):\n","        chunk = leaves[i:i + max_objects_per_part]\n","        partial: JsonDict = {}\n","        for path, leaf in chunk:\n","            insert_path(partial, path, leaf, dedup=dedup_consecutive)\n","        parts.append(partial)\n","\n","    return parts\n","\n","\n","def split_json_file_by_measurements(\n","    file_path: Union[str, Path],\n","    max_objects_per_part: int,\n","    output_dir: Union[str, Path, None] = None,\n","    dedup_consecutive: bool = True,\n","    indent: int = 2,\n",") -> List[str]:\n","    \"\"\"\n","    Charge un fichier JSON objet/arborescent, le découpe par feuilles (mesures),\n","    écrit éventuellement les morceaux et renvoie la liste des **textes JSON**.\n","    \"\"\"\n","    p = Path(file_path)\n","    data = json.loads(Path(file_path).read_text(encoding=\"utf-8\"))\n","\n","    parts = split_dict_by_measurements(\n","        data, max_objects_per_part=max_objects_per_part, dedup_consecutive=dedup_consecutive\n","    )\n","\n","    json_texts: List[str] = []\n","    for idx, part in enumerate(parts, start=1):\n","        text = json.dumps(part, ensure_ascii=False, indent=indent)\n","        json_texts.append(text)\n","\n","        if output_dir is not None:\n","            Path(output_dir).mkdir(parents=True, exist_ok=True)\n","            out_path = Path(output_dir) / f\"{p.stem}_byLeaf_part{idx}.json\"\n","            out_path.write_text(text, encoding=\"utf-8\")\n","\n","    return json_texts\n","\n","\n","\n"],"metadata":{"id":"nusKbHh4QYsO","executionInfo":{"status":"ok","timestamp":1755455651242,"user_tz":-120,"elapsed":25,"user":{"displayName":"Saima Ben Hadj","userId":"02052565308082602743"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","if __name__ == \"__main__\":\n","    # Suppose que 'rapport.json' contient ta structure arborescente\n","    parts_text = split_json_file_by_measurements(\n","        \"/content/sample_data/52_annotation_empty.json\",\n","        max_objects_per_part=5,   # <- nb max de mesures par fichier\n","        output_dir=\"out_parts\",    # optionnel\n","        dedup_consecutive=True     # enlève les \"Hormonologie\" imbriqués en doublon\n","    )\n","\n","    print(f\"Generated {len(parts_text)} parts.\")\n","    for i, t in enumerate(parts_text, 1):\n","        print(\"\\n*******************\")\n","        print(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZD5EjMMgQhMP","executionInfo":{"status":"ok","timestamp":1755455762584,"user_tz":-120,"elapsed":28,"user":{"displayName":"Saima Ben Hadj","userId":"02052565308082602743"}},"outputId":"7f29570f-2d1b-493c-88fa-89a8c166ee68"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated 7 parts.\n","\n","*******************\n","{\n","  \"Hematologie\": {\n","    \"NumerationGlobulaire\": {\n","      \"Hematies\": {\n","        \"valeur\": null,\n","        \"unité\": \"T/L\"\n","      },\n","      \"Hematocrite\": {\n","        \"valeur\": null,\n","        \"unité\": \"%\"\n","      },\n","      \"CONCENTRATION GLOB.MOYENNE (CCMH)\": {\n","        \"valeur\": null,\n","        \"unité\": \"g/dL\"\n","      }\n","    },\n","    \"FormuleLeucocytaire\": {\n","      \"Leucocytes\": {\n","        \"valeur\": null,\n","        \"unité\": \"G/L\"\n","      },\n","      \"Neutrophiles\": {\n","        \"valeur\": null,\n","        \"unité\": \"%\"\n","      }\n","    }\n","  }\n","}\n","\n","*******************\n","{\n","  \"Hematologie\": {\n","    \"FormuleLeucocytaire\": {\n","      \"Eosinophiles\": {\n","        \"valeur\": null,\n","        \"unité\": \"%\"\n","      },\n","      \"Basophiles\": {\n","        \"valeur\": null,\n","        \"unité\": \"%\"\n","      },\n","      \"Lymphocytes\": {\n","        \"valeur\": null,\n","        \"unité\": \"%\"\n","      },\n","      \"Monocytes\": {\n","        \"valeur\": null,\n","        \"unité\": \"%\"\n","      }\n","    },\n","    \"NumerationPlaquettaire\": {\n","      \"Plaquettes\": {\n","        \"valeur\": null,\n","        \"unité\": \"G/L\"\n","      }\n","    }\n","  }\n","}\n","\n","*******************\n","{\n","  \"Hemostase\": {\n","    \"Taux de prothrombine\": {\n","      \"valeur\": null,\n","      \"unité\": \"%\"\n","    },\n","    \"INR\": {\n","      \"valeur\": null,\n","      \"unité\": \"\"\n","    }\n","  },\n","  \"Biochimie\": {\n","    \"Sodium\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    },\n","    \"DEBIT DE FILTRATION GLOMERULAIRE (MDRD)\": {\n","      \"valeur\": null,\n","      \"unité\": \"ml/mn/1.73m²\"\n","    },\n","    \"Urée\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    }\n","  }\n","}\n","\n","*******************\n","{\n","  \"Biochimie\": {\n","    \"Transaminase ALAT (S.G.P.T)\": {\n","      \"valeur\": null,\n","      \"unité\": \"U/L\"\n","    },\n","    \"Glycémie à jeun\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    },\n","    \"Cholestérol H.D.L.\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    },\n","    \"Cholestérol L.D.L.\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    },\n","    \"Triglycérides\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    }\n","  }\n","}\n","\n","*******************\n","{\n","  \"Biochimie\": {\n","    \"Cholestérol total\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    },\n","    \"Hémoglobine glyquée\": {\n","      \"valeur\": null,\n","      \"unité\": \"%\"\n","    },\n","    \"Chlorure\": {\n","      \"valeur\": null,\n","      \"unité\": \"mmol/L\"\n","    },\n","    \"Protides totaux\": {\n","      \"valeur\": null,\n","      \"unité\": \"g/L\"\n","    },\n","    \"Créatine Phospho Kinase (CPK)\": {\n","      \"valeur\": null,\n","      \"unité\": \"UI/L\"\n","    }\n","  }\n","}\n","\n","*******************\n","{\n","  \"Biochimie\": {\n","    \"Lactate Déshydrogénase (LDH)\": {\n","      \"valeur\": null,\n","      \"unité\": \"UI/L\"\n","    }\n","  },\n","  \"CytologieUrinaire\": {\n","    \"LEUCOCYTES\": {\n","      \"valeur\": null,\n","      \"unité\": \"/mL\"\n","    },\n","    \"Neuron Specific Enolase\": {\n","      \"valeur\": null,\n","      \"unité\": \"ng/mL\"\n","    }\n","  },\n","  \"Hormonologie\": {\n","    \"T.S.H. ultra-sensible\": {\n","      \"valeur\": null,\n","      \"unité\": \"mUI/L\"\n","    },\n","    \"Thyroxine libre (T4L)\": {\n","      \"valeur\": null,\n","      \"unité\": \"pmol/L\"\n","    }\n","  }\n","}\n","\n","*******************\n","{\n","  \"Hormonologie\": {\n","    \"Insuline\": {\n","      \"valeur\": null,\n","      \"unité\": \"mU/L\"\n","    },\n","    \"Index HOMA\": {\n","      \"valeur\": null,\n","      \"unité\": \"\"\n","    },\n","    \"Testostérone\": {\n","      \"valeur\": null,\n","      \"unité\": \"ng/mL\"\n","    }\n","  }\n","}\n"]}]}]}